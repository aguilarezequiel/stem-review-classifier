{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 - Evaluación del Modelo en el Conjunto de Test\n",
    "\n",
    "**Materia:** Redes Neuronales Profundas — UTN FRM\n",
    "\n",
    "**Objetivo:** Evaluar el modelo fine-tuneado sobre el conjunto de test, calcular métricas de clasificación y realizar pruebas cualitativas.\n",
    "\n",
    "---\n",
    "\n",
    "## Métricas de Evaluación\n",
    "\n",
    "- **Accuracy:** Proporción de predicciones correctas sobre el total.\n",
    "- **Precision:** De los predichos como positivos, ¿cuántos realmente lo son?\n",
    "- **Recall:** De los positivos reales, ¿cuántos detectó el modelo?\n",
    "- **F1-Score:** Media armónica de Precision y Recall.\n",
    "- **MCC (Matthews Correlation Coefficient):** Métrica robusta que considera las 4 celdas de la matriz de confusión. Rango -1 a +1.\n",
    "- **Matriz de Confusión:** Tabla con TP, TN, FP, FN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importación de Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\nimport numpy as np\nimport pandas as pd\nimport torch\n",
    "from torch.utils.data import DataLoader, SequentialSampler\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, matthews_corrcoef, accuracy_score\n",
    "import matplotlib.pyplot as plt\nimport seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TENSORS_DIR = \"../data/tensors/\"\nMODEL_DIR = \"../data/model_save/\"\nRESULTS_DIR = \"../data/results/\"\nBATCH_SIZE = 32\n",
    "\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Dispositivo: {device}\")\n",
    "if torch.cuda.is_available(): print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Carga del Modelo Fine-Tuneado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Cargando modelo desde {MODEL_DIR}...\")\n",
    "model = BertForSequenceClassification.from_pretrained(MODEL_DIR)\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_DIR)\n",
    "model.to(device)\n",
    "print(f\"Modelo cargado: {type(model).__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Carga del Conjunto de Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = torch.load(os.path.join(TENSORS_DIR, \"test_dataset.pt\"), weights_only=False)\n",
    "print(f\"Test: {len(test_dataset):,} muestras\")\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, sampler=SequentialSampler(test_dataset), batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generación de Predicciones\n",
    "\n",
    "Recorremos todos los batches en modo evaluación (`model.eval()`) sin gradientes (`torch.no_grad()`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Prediciendo para {len(test_dataset):,} muestras...\")\nmodel.eval()\n\npredictions = []\ntrue_labels = []\n",
    "\nfor batch in test_dataloader:\n",
    "    b_input_ids = batch[0].to(device)\n",
    "    b_input_mask = batch[1].to(device)\n",
    "    b_labels = batch[2].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        result = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, return_dict=True)\n",
    "\n",
    "    predictions.append(result.logits.detach().cpu().numpy())\n",
    "    true_labels.append(b_labels.to('cpu').numpy())\n",
    "\n",
    "flat_predictions = np.argmax(np.concatenate(predictions, axis=0), axis=1).flatten()\n",
    "flat_true_labels = np.concatenate(true_labels, axis=0)\n",
    "print(\"Predicción completa.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Cálculo de Métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = accuracy_score(flat_true_labels, flat_predictions)\n",
    "mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n",
    "print(f\"Accuracy: {acc:.4f}\")\n",
    "print(f\"MCC: {mcc:.4f}\")\n",
    "\n",
    "target_names = ['Negativa', 'Positiva']\n",
    "report = classification_report(flat_true_labels, flat_predictions, target_names=target_names)\n",
    "print(f\"\\nClassification Report:\\n{report}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Matriz de Confusión\n",
    "\n",
    "Visualiza TN (verdaderos negativos), FP (falsos positivos), FN (falsos negativos) y TP (verdaderos positivos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(flat_true_labels, flat_predictions)\n",
    "\nplt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "    xticklabels=['Negativa', 'Positiva'], yticklabels=['Negativa', 'Positiva'])\n",
    "plt.ylabel('Etiqueta Real')\nplt.xlabel('Predicción')\n",
    "plt.title('Matriz de Confusión - Steam Review Classifier')\nplt.tight_layout()\n",
    "\nos.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "plt.savefig(os.path.join(RESULTS_DIR, \"confusion_matrix.png\"), dpi=150)\nplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Curvas de Pérdida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_path = os.path.join(MODEL_DIR, \"training_stats.csv\")\n",
    "if os.path.exists(stats_path):\n",
    "    df_stats = pd.read_csv(stats_path)\n",
    "    sns.set(style='darkgrid'); sns.set(font_scale=1.5)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(df_stats['epoch'], df_stats['Training Loss'], 'b-o', label='Entrenamiento')\n",
    "    plt.plot(df_stats['epoch'], df_stats['Valid. Loss'], 'g-o', label='Validación')\n",
    "    plt.title('Pérdida de Entrenamiento y Validación')\n",
    "    plt.xlabel('Época'); plt.ylabel('Pérdida'); plt.legend()\n",
    "    plt.xticks(df_stats['epoch']); plt.tight_layout()\n",
    "    plt.savefig(os.path.join(RESULTS_DIR, 'training_loss.png'), dpi=150)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Pruebas Cualitativas\n",
    "\n",
    "Probamos con reseñas de ejemplo para verificar el comportamiento del modelo más allá de las métricas numéricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    \"This game is absolutely amazing, I love the graphics and gameplay!\",\n",
    "    \"Terrible game, crashes every 5 minutes. Do not buy this garbage.\",\n",
    "    \"It's okay, nothing special but not bad either. Average game.\",\n",
    "    \"Best game I have ever played. Hundreds of hours of fun!\",\n",
    "    \"Waste of money. The developers abandoned this project.\",\n",
    "    \"Really fun with friends, great multiplayer experience.\",\n",
    "]\n",
    "\nprint(\"=\" * 50)\nprint(\"  PRUEBAS CUALITATIVAS\")\nprint(\"=\" * 50)\n",
    "\nmodel.eval()\n",
    "for text in examples:\n",
    "    encoded = tokenizer(text, add_special_tokens=True, max_length=128, padding='max_length',\n",
    "                        truncation=True, return_attention_mask=True, return_tensors='pt')\n",
    "    input_ids = encoded['input_ids'].to(device)\n",
    "    attention_mask = encoded['attention_mask'].to(device)\n",
    "    with torch.no_grad():\n",
    "        result = model(input_ids, attention_mask=attention_mask, return_dict=True)\n",
    "    probs = torch.softmax(result.logits, dim=1)\n",
    "    pred = torch.argmax(probs, dim=1).item()\n",
    "    confidence = probs[0][pred].item()\n",
    "    label = 'POSITIVA' if pred == 1 else 'NEGATIVA'\n",
    "    print(f'\\n  [{label}] (confianza: {confidence:.2%})')\n",
    "    print(f'  \"{text}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Guardado de Métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(RESULTS_DIR, 'metrics.txt'), 'w') as f:\n",
    "    f.write(f'Accuracy: {acc:.4f}\\n')\n",
    "    f.write(f'MCC: {mcc:.4f}\\n\\n')\n",
    "    f.write(f'Classification Report:\\n{report}\\n')\n",
    "    f.write(f'Confusion Matrix:\\n{cm}\\n')\n",
    "print(f\"Resultados guardados en: {RESULTS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumen\n",
    "\n",
    "### Resultados en Test:\n",
    "\n",
    "| Métrica | Valor |\n",
    "|---|---|\n",
    "| **Accuracy** | 90.56% |\n",
    "| **MCC** | 0.8112 |\n",
    "| **Precision (Neg/Pos)** | 0.91 / 0.90 |\n",
    "| **Recall (Neg/Pos)** | 0.90 / 0.91 |\n",
    "| **F1-Score (macro)** | 0.91 |\n",
    "\n",
    "El modelo alcanza 90.56% de accuracy y MCC de 0.81, con métricas equilibradas entre clases. Las pruebas cualitativas confirman clasificación correcta.\n",
    "\n",
    "**Siguiente paso:** Exportar el modelo para producción."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}