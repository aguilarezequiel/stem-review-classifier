{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - Descarga y Limpieza del Dataset de Steam Reviews\n",
    "\n",
    "**Materia:** Redes Neuronales Profundas — UTN FRM\n",
    "\n",
    "**Objetivo:** Descargar el dataset de reseñas de Steam desde Kaggle, realizar una limpieza exhaustiva del texto y preparar un CSV balanceado para las etapas posteriores de tokenización y entrenamiento.\n",
    "\n",
    "---\n",
    "\n",
    "## Descripción del Dataset\n",
    "\n",
    "Utilizamos el dataset [Steam Reviews](https://www.kaggle.com/datasets/andrewmvd/steam-reviews) de Kaggle (autor: andrewmvd). Este dataset contiene reseñas reales de usuarios de la plataforma Steam.\n",
    "\n",
    "**Columnas relevantes:**\n",
    "- `review_text`: texto libre de la reseña del usuario\n",
    "- `review_score`: sentimiento de la reseña (1 = positivo, -1 = negativo)\n",
    "\n",
    "**Tarea:** Clasificación binaria de sentimiento (positivo vs negativo).\n",
    "\n",
    "## ¿Por qué limpiar el texto?\n",
    "\n",
    "Las reseñas de Steam contienen gran cantidad de \"ruido\" que puede confundir al modelo:\n",
    "- Arte ASCII (dibujos con caracteres especiales)\n",
    "- URLs y enlaces\n",
    "- Emotes con asteriscos (\\*joins server\\*)\n",
    "- Puntuación excesiva (!!!!!!)\n",
    "- Caracteres especiales y Unicode\n",
    "\n",
    "Una limpieza agresiva del texto permite que BERT se enfoque en las palabras reales que expresan sentimiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importación de Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\nimport re\nimport pandas as pd\nimport numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuración de Parámetros\n",
    "\n",
    "- **SAMPLE_SIZE = 50,000:** Tomamos 25,000 reseñas positivas y 25,000 negativas para tener un dataset balanceado.\n",
    "- **MIN_WORD_COUNT = 5:** Descartamos reseñas con menos de 5 palabras (no aportan información útil).\n",
    "- **MAX_WORD_COUNT = 200:** Limitamos la longitud para evitar reseñas extremadamente largas que podrían ser spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_DATA_PATH = \"../data/dataset.csv\"        # CSV descargado de Kaggle\n",
    "CLEAN_DATA_PATH = \"../data/clean_reviews.csv\"  # CSV limpio de salida\n",
    "SAMPLE_SIZE = 50000                             # 25K positivas + 25K negativas\n",
    "MIN_WORD_COUNT = 5                              # Mínimo de palabras por reseña\n",
    "MAX_WORD_COUNT = 200                            # Máximo de palabras por reseña\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Carga del Dataset Crudo\n",
    "\n",
    "El dataset de Kaggle puede venir en uno o más archivos CSV. Esta función maneja ambos casos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_raw_data(path):\n",
    "    \"\"\"Carga el CSV crudo del dataset de Kaggle.\"\"\"\n",
    "    if os.path.isfile(path):\n",
    "        print(f\"Cargando datos desde {path}...\")\n",
    "        df = pd.read_csv(path)\n",
    "    else:\n",
    "        data_dir = os.path.dirname(path)\n",
    "        csv_files = [f for f in os.listdir(data_dir) if f.endswith('.csv') and 'clean' not in f]\n",
    "        if not csv_files:\n",
    "            raise FileNotFoundError(\n",
    "                f\"No se encontraron archivos CSV en {data_dir}.\\n\"\n",
    "                \"Descargá el dataset con: kaggle datasets download -d andrewmvd/steam-reviews\"\n",
    "            )\n",
    "        print(f\"Archivos encontrados: {csv_files}\")\n",
    "        dfs = [pd.read_csv(os.path.join(data_dir, f)) for f in csv_files]\n",
    "        df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    print(f\"Datos cargados: {df.shape[0]:,} filas, {df.shape[1]} columnas\")\n",
    "    print(f\"Columnas: {list(df.columns)}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_raw_data(RAW_DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploración Inicial del Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Forma: {df.shape}\")\nprint(f\"\\nTipos de datos:\\n{df.dtypes}\")\nprint(f\"\\nValores nulos:\\n{df.isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Distribución de review_score:\")\nprint(df['review_score'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Funciones de Limpieza de Texto\n",
    "\n",
    "### 4.1 Función `clean_text`\n",
    "\n",
    "Aplica una limpieza agresiva al texto de cada reseña:\n",
    "\n",
    "1. **Eliminar URLs:** Los enlaces no aportan información de sentimiento.\n",
    "2. **Eliminar arte ASCII:** Caracteres decorativos comunes en Steam (░▒▓█).\n",
    "3. **Eliminar emotes con asteriscos:** Patrones como \\*applauds\\*.\n",
    "4. **Normalizar puntuación:** Reducir repeticiones excesivas (!!!!! → !!).\n",
    "5. **Eliminar caracteres especiales:** Mantener solo letras, espacios y puntuación básica.\n",
    "6. **Convertir a minúsculas:** Necesario para `bert-base-uncased`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"Limpia agresivamente el texto de una reseña de Steam.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = re.sub(r'http\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub(r'[░▒▓█▄▀■□▪▫●○◆◇♠♣♥♦♪♫☆★►◄▲▼←→↑↓]+', '', text)\n",
    "    text = re.sub(r'\\*[^*]+\\*', '', text)\n",
    "    text = re.sub(r'[•►▪▸‣⁃]', '', text)\n",
    "    text = re.sub(r'(\\d+)\\s*/\\s*(\\d+)', r'\\1 out of \\2', text)\n",
    "    text = re.sub(r'\\d{3,4}\\s*x\\s*\\d{3,4}', '', text)\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "    text = re.sub(r'\\b\\d+\\b', '', text)\n",
    "    text = re.sub(r'([!?.]){3,}', r'\\1\\1', text)\n",
    "    text = re.sub(r\"[^a-zA-Z\\s.,!?'-]\", ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = text.strip().lower()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Función de Validación\n",
    "\n",
    "Filtra reseñas que no son útiles para entrenamiento: muy cortas (< 5 palabras), muy largas (> 200 palabras), o con muy pocos caracteres únicos (spam)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_review(text, min_words=MIN_WORD_COUNT, max_words=MAX_WORD_COUNT):\n",
    "    \"\"\"Verifica si una reseña limpia es válida para entrenamiento.\"\"\"\n",
    "    if not isinstance(text, str) or len(text.strip()) == 0:\n",
    "        return False\n",
    "    words = text.split()\n",
    "    if len(words) < min_words or len(words) > max_words:\n",
    "        return False\n",
    "    if len(set(text.replace(' ', ''))) < 5:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo de Limpieza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ejemplo_raw = df.iloc[0]['review_text'] if 'review_text' in df.columns else df.iloc[0][df.columns[0]]\nejemplo_clean = clean_text(str(ejemplo_raw))\nprint(\"ORIGINAL:\")\nprint(str(ejemplo_raw)[:300])\nprint(\"\\nLIMPIO:\")\nprint(ejemplo_clean[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Procesamiento Completo y Balanceo de Clases\n",
    "\n",
    "### ¿Por qué balancear clases?\n",
    "\n",
    "Un dataset desbalanceado puede sesgar al modelo hacia la clase mayoritaria. Al tener exactamente 25,000 muestras de cada clase, el modelo aprende igualmente bien a detectar ambos sentimientos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_save(df, output_path, sample_size, seed):\n",
    "    \"\"\"Procesa el DataFrame: limpia, filtra, balancea y guarda.\"\"\"\n",
    "    print(\"--- Limpieza de texto ---\")\n",
    "    text_col, score_col = None, None\n",
    "    for col in df.columns:\n",
    "        col_lower = col.lower()\n",
    "        if 'review_text' in col_lower: text_col = col\n",
    "        if 'review_score' in col_lower: score_col = col\n",
    "    if text_col is None: text_col = 'review_text'\n",
    "    if score_col is None: score_col = 'review_score'\n",
    "    print(f\"Columna de texto: {text_col}, Columna de score: {score_col}\")\n",
    "\n",
    "    df = df.dropna(subset=[text_col])\n",
    "    print(f\"Filas después de eliminar nulos: {df.shape[0]:,}\")\n",
    "\n",
    "    print(\"Limpiando texto...\")\n",
    "    df['clean_text'] = df[text_col].apply(clean_text)\n",
    "    df['is_valid'] = df['clean_text'].apply(is_valid_review)\n",
    "    df = df[df['is_valid']].copy()\n",
    "    print(f\"Filas después de filtrar inválidas: {df.shape[0]:,}\")\n",
    "\n",
    "    if df[score_col].dtype == object:\n",
    "        df['label'] = (df[score_col].str.lower() == 'recommended').astype(int)\n",
    "    else:\n",
    "        df['label'] = (df[score_col] > 0).astype(int)\n",
    "\n",
    "    print(f\"\\nDistribución: Pos={df['label'].sum():,}, Neg={(df['label']==0).sum():,}\")\n",
    "\n",
    "    half = sample_size // 2\n",
    "    pos = df[df['label'] == 1]\n",
    "    neg = df[df['label'] == 0]\n",
    "    n = min(half, len(pos), len(neg))\n",
    "    print(f\"Muestreando {n:,} de cada clase (total: {n*2:,})\")\n",
    "\n",
    "    balanced = pd.concat([\n",
    "        pos.sample(n=n, random_state=seed),\n",
    "        neg.sample(n=n, random_state=seed)\n",
    "    ]).sample(frac=1, random_state=seed)\n",
    "\n",
    "    result = balanced[['clean_text', 'label']].reset_index(drop=True)\n",
    "    result.columns = ['text', 'label']\n",
    "    result.to_csv(output_path, index=False)\n",
    "\n",
    "    print(f\"\\nGuardado: {output_path}\")\n",
    "    print(f\"Total: {len(result):,}, Distribución: {result['label'].value_counts().to_dict()}\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = process_and_save(df, CLEAN_DATA_PATH, SAMPLE_SIZE, RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Verificación del Dataset Limpio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in result.head(5).iterrows():\n    label_str = \"POSITIVA\" if row['label'] == 1 else \"NEGATIVA\"\n    print(f\"[{label_str}] {row['text'][:150]}...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['word_count'] = result['text'].apply(lambda x: len(x.split()))\nprint(\"Estadísticas de longitud (palabras):\")\nprint(result['word_count'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumen\n",
    "\n",
    "1. Cargamos el dataset crudo de Steam Reviews de Kaggle.\n",
    "2. Aplicamos limpieza agresiva del texto (URLs, arte ASCII, caracteres especiales, etc.).\n",
    "3. Filtramos reseñas inválidas (muy cortas, muy largas, spam).\n",
    "4. Balanceamos las clases (25,000 positivas + 25,000 negativas).\n",
    "5. Guardamos en `data/clean_reviews.csv`.\n",
    "\n",
    "**Siguiente paso:** Tokenizar las reseñas usando el tokenizer de BERT."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}