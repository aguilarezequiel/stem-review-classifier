{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 - Tokenización y Creación del Dataset en PyTorch\n",
    "\n",
    "**Materia:** Redes Neuronales Profundas — UTN FRM\n",
    "\n",
    "**Objetivo:** Tokenizar las reseñas limpias usando el tokenizer de BERT (`bert-base-uncased`), crear los TensorDatasets y dividir en conjuntos de entrenamiento, validación y test.\n",
    "\n",
    "---\n",
    "\n",
    "## ¿Qué es la tokenización en BERT?\n",
    "\n",
    "BERT no recibe texto crudo como entrada. El texto debe convertirse en una secuencia de tokens numéricos. El tokenizer de BERT realiza:\n",
    "\n",
    "1. **Tokenización WordPiece:** Divide palabras en sub-palabras. Ej: \"playing\" → [\"play\", \"##ing\"]. Esto permite manejar palabras desconocidas.\n",
    "2. **Tokens especiales:**\n",
    "   - `[CLS]` al inicio: su representación final se usa como embedding de la oración completa para clasificación.\n",
    "   - `[SEP]` al final: marca el fin de la secuencia.\n",
    "3. **Mapeo a IDs:** Cada token se convierte a su ID numérico en el vocabulario de BERT (30,522 tokens).\n",
    "4. **Padding:** Se rellena con `[PAD]` (ID=0) hasta alcanzar `max_length`.\n",
    "5. **Attention Mask:** Vector binario que indica cuáles tokens son reales (1) y cuáles son padding (0).\n",
    "\n",
    "### Ejemplo visual:\n",
    "```\n",
    "Texto:    \"great game\"\n",
    "Tokens:   [CLS] great game [SEP] [PAD] [PAD] ...\n",
    "IDs:      101   2307  2208  102   0     0     ...\n",
    "Mask:     1     1     1     1     0     0     ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importación de Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import TensorDataset, random_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuración\n",
    "\n",
    "- **MAX_LENGTH = 128:** BERT soporta hasta 512 tokens, pero 128 es suficiente para la mayoría de las reseñas y reduce memoria y tiempo.\n",
    "- **MODEL_NAME = 'bert-base-uncased':** Modelo BERT base en inglés, sin distinción de mayúsculas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLEAN_DATA_PATH = \"../data/clean_reviews.csv\"\n",
    "TENSORS_DIR = \"../data/tensors/\"\n",
    "MAX_LENGTH = 128\n",
    "MODEL_NAME = 'bert-base-uncased'\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Carga del Dataset Limpio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(CLEAN_DATA_PATH)\n",
    "print(f\"Dataset cargado: {len(df):,} muestras\")\n",
    "print(f\"Distribución: {df['label'].value_counts().to_dict()}\")\n",
    "\n",
    "sentences = df['text'].values\n",
    "labels = df['label'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Carga del Tokenizer de BERT\n",
    "\n",
    "Usamos `BertTokenizer.from_pretrained()` para cargar el tokenizer preentrenado. El parámetro `do_lower_case=True` indica que el texto se convertirá a minúsculas (consistente con `bert-base-uncased`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Cargando tokenizer de BERT ({MODEL_NAME})...\")\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME, do_lower_case=True)\n",
    "print(f\"Vocabulario: {tokenizer.vocab_size:,} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo de Tokenización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ejemplo = sentences[1]\n",
    "print(f\"Texto original: {ejemplo}\")\n",
    "print(f\"\\nTokens: {tokenizer.tokenize(ejemplo)[:20]}...\")\n",
    "\n",
    "encoded = tokenizer(\n",
    "    ejemplo,\n",
    "    add_special_tokens=True,\n",
    "    max_length=MAX_LENGTH,\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    return_attention_mask=True,\n",
    "    return_tensors='pt',\n",
    ")\n",
    "print(f\"\\nInput IDs (primeros 20): {encoded['input_ids'][0][:20].tolist()}\")\n",
    "print(f\"Attention Mask (primeros 20): {encoded['attention_mask'][0][:20].tolist()}\")\n",
    "print(f\"Longitud total: {encoded['input_ids'].shape[1]} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Tokenización de Todo el Dataset\n",
    "\n",
    "Aplicamos la tokenización a todas las reseñas. Para cada una, el tokenizer:\n",
    "1. Agrega `[CLS]` y `[SEP]` (`add_special_tokens=True`)\n",
    "2. Trunca a `max_length` si excede (`truncation=True`)\n",
    "3. Rellena con `[PAD]` hasta `max_length` (`padding='max_length'`)\n",
    "4. Genera la attention mask (`return_attention_mask=True`)\n",
    "\n",
    "El resultado son tres tensores: **input_ids**, **attention_masks** y **labels**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "print(f\"Tokenizando {len(sentences):,} reseñas...\")\n",
    "\n",
    "for i, sent in enumerate(sentences):\n",
    "    if (i + 1) % 10000 == 0:\n",
    "        print(f\"  Procesadas {i+1:,} / {len(sentences):,}\")\n",
    "\n",
    "    encoded_dict = tokenizer(\n",
    "        sent,\n",
    "        add_special_tokens=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt',\n",
    "    )\n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# Concatenar en tensores únicos\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "print(f\"\\nTokenización completa.\")\n",
    "print(f\"  input_ids shape:      {input_ids.shape}\")\n",
    "print(f\"  attention_masks shape: {attention_masks.shape}\")\n",
    "print(f\"  labels shape:          {labels_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. División en Train / Validation / Test\n",
    "\n",
    "- **Entrenamiento (80%):** Para ajustar los pesos del modelo.\n",
    "- **Validación (10%):** Para monitorear rendimiento y evitar overfitting.\n",
    "- **Test (10%):** Evaluación final (nunca visto durante el entrenamiento).\n",
    "\n",
    "Usamos `TensorDataset` y `random_split` de PyTorch, como en el notebook de clase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(input_ids, attention_masks, labels_tensor)\n",
    "\n",
    "total = len(dataset)\n",
    "train_size = int(0.8 * total)\n",
    "val_size = int(0.1 * total)\n",
    "test_size = total - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(\n",
    "    dataset, [train_size, val_size, test_size],\n",
    "    generator=torch.Generator().manual_seed(RANDOM_SEED)\n",
    ")\n",
    "\n",
    "print(f\"Entrenamiento: {train_size:,} ({train_size/total:.0%})\")\n",
    "print(f\"Validación:    {val_size:,} ({val_size/total:.0%})\")\n",
    "print(f\"Test:          {test_size:,} ({test_size/total:.0%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Guardado de los Datasets\n",
    "\n",
    "Guardamos en dos formatos:\n",
    "1. **Tensores `.pt`:** Para cargar directamente en entrenamiento con `torch.load()`.\n",
    "2. **Archivos `.csv`:** Documentación de los splits con el texto original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(TENSORS_DIR, exist_ok=True)\n",
    "torch.save(train_dataset, os.path.join(TENSORS_DIR, \"train_dataset.pt\"))\n",
    "torch.save(val_dataset, os.path.join(TENSORS_DIR, \"val_dataset.pt\"))\n",
    "torch.save(test_dataset, os.path.join(TENSORS_DIR, \"test_dataset.pt\"))\n",
    "print(f\"Datasets .pt guardados en: {TENSORS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.dirname(CLEAN_DATA_PATH)\n",
    "df.iloc[train_dataset.indices].reset_index(drop=True).to_csv(os.path.join(data_dir, \"training_data.csv\"), index=False)\n",
    "df.iloc[val_dataset.indices].reset_index(drop=True).to_csv(os.path.join(data_dir, \"validation_data.csv\"), index=False)\n",
    "df.iloc[test_dataset.indices].reset_index(drop=True).to_csv(os.path.join(data_dir, \"test_data.csv\"), index=False)\n",
    "print(f\"CSVs guardados en: {data_dir}\")\n",
    "print(f\"  training_data.csv   ({len(train_dataset.indices):,} filas)\")\n",
    "print(f\"  validation_data.csv ({len(val_dataset.indices):,} filas)\")\n",
    "print(f\"  test_data.csv       ({len(test_dataset.indices):,} filas)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumen\n",
    "\n",
    "1. Cargamos las 50,000 reseñas limpias.\n",
    "2. Tokenizamos con `BertTokenizer`: tokens especiales `[CLS]`/`[SEP]`, padding a 128, attention masks.\n",
    "3. Creamos `TensorDataset` con (input_ids, attention_masks, labels).\n",
    "4. Dividimos en Train (80%), Validation (10%) y Test (10%).\n",
    "5. Guardamos como `.pt` y `.csv`.\n",
    "\n",
    "**Siguiente paso:** Fine-tuning del modelo BERT."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}