{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05 - Exportar Modelo para Producción\n",
    "\n",
    "**Materia:** Redes Neuronales Profundas — UTN FRM\n",
    "\n",
    "**Objetivo:** Exportar el modelo fine-tuneado para desplegarlo en la aplicación web de Streamlit.\n",
    "\n",
    "---\n",
    "\n",
    "## Formatos de Exportación\n",
    "\n",
    "1. **`modelo.pth`:** Solo el state_dict (pesos). Formato liviano.\n",
    "2. **`model_files/`:** Carpeta con todo lo necesario para `from_pretrained()`:\n",
    "   - `model.safetensors`: Pesos en formato seguro.\n",
    "   - `config.json`: Configuración del modelo.\n",
    "   - `tokenizer.json` y `tokenizer_config.json`: Vocabulario y configuración del tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importación de Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\nimport torch\nfrom transformers import BertForSequenceClassification, BertTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = \"../data/model_save/\"\nPROD_DIR = \"../prod/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Carga y Exportación del Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Cargando modelo desde {MODEL_DIR}...\")\n",
    "model = BertForSequenceClassification.from_pretrained(MODEL_DIR)\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_DIR)\n",
    "os.makedirs(PROD_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar state_dict como .pth (liviano para producción)\n",
    "pth_path = os.path.join(PROD_DIR, \"modelo.pth\")\n",
    "torch.save(model.state_dict(), pth_path)\n",
    "file_size = os.path.getsize(pth_path) / (1024 * 1024)\n",
    "print(f\"Modelo guardado: {pth_path} ({file_size:.1f} MB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar modelo completo con save_pretrained\n",
    "model_prod_dir = os.path.join(PROD_DIR, \"model_files\")\n",
    "os.makedirs(model_prod_dir, exist_ok=True)\n",
    "model.save_pretrained(model_prod_dir)\n",
    "tokenizer.save_pretrained(model_prod_dir)\n",
    "print(f\"Modelo y tokenizer en: {model_prod_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Verificación\n",
    "\n",
    "Verificamos que el modelo exportado se puede cargar correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Verificando carga...\")\n",
    "loaded_model = BertForSequenceClassification.from_pretrained(model_prod_dir)\n",
    "loaded_tokenizer = BertTokenizer.from_pretrained(model_prod_dir)\n",
    "print(f\"Modelo: {type(loaded_model).__name__}\")\n",
    "print(f\"Tokenizer vocab: {loaded_tokenizer.vocab_size}\")\n",
    "\nprint(f\"\\nArchivos en {model_prod_dir}:\")\n",
    "for f in os.listdir(model_prod_dir):\n",
    "    size = os.path.getsize(os.path.join(model_prod_dir, f)) / (1024*1024)\n",
    "    print(f\"  {f} ({size:.2f} MB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prueba rápida\n",
    "test_text = \"This game is fantastic, highly recommend!\"\n",
    "encoded = loaded_tokenizer(test_text, add_special_tokens=True, max_length=128,\n",
    "                           padding='max_length', truncation=True, return_tensors='pt')\n",
    "\nloaded_model.eval()\n",
    "with torch.no_grad():\n",
    "    result = loaded_model(**encoded)\n",
    "\nprobs = torch.softmax(result.logits, dim=1)\n",
    "pred = torch.argmax(probs, dim=1).item()\n",
    "confidence = probs[0][pred].item()\n",
    "label = 'POSITIVA' if pred == 1 else 'NEGATIVA'\n",
    "\nprint(f\"\\nPrueba: '{test_text}'\")\n",
    "print(f\"Predicción: {label} (confianza: {confidence:.2%})\")\n",
    "print(f\"\\nExportación completada. Modelo listo para producción.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumen\n",
    "\n",
    "1. Cargamos el modelo fine-tuneado.\n",
    "2. Exportamos como `modelo.pth` (state_dict).\n",
    "3. Exportamos modelo completo en `model_files/` con `save_pretrained()`.\n",
    "4. Verificamos carga correcta y realizamos prueba.\n",
    "\n",
    "**El modelo está listo para la aplicación web de Streamlit (`prod/app.py`).**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}