{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 - Entrenamiento del Modelo (Fine-Tuning de BERT)\n",
    "\n",
    "**Materia:** Redes Neuronales Profundas — UTN FRM\n",
    "\n",
    "**Objetivo:** Realizar el fine-tuning de `BertForSequenceClassification` sobre el dataset de Steam Reviews para clasificación binaria de sentimiento.\n",
    "\n",
    "---\n",
    "\n",
    "## ¿Qué es el Fine-Tuning?\n",
    "\n",
    "El fine-tuning es tomar un modelo preentrenado (que aprendió representaciones generales del lenguaje) y adaptarlo a una tarea específica.\n",
    "\n",
    "**BERT** fue preentrenado en dos tareas:\n",
    "1. **Masked Language Model (MLM):** Predecir palabras enmascaradas.\n",
    "2. **Next Sentence Prediction (NSP):** Predecir si dos oraciones son consecutivas.\n",
    "\n",
    "Durante el fine-tuning:\n",
    "- Se agrega una **capa de clasificación** lineal encima de BERT (toma el embedding de `[CLS]` y produce 2 salidas).\n",
    "- Se entrenan **todos los parámetros** con un learning rate muy bajo (2e-5) para no destruir el conocimiento preentrenado.\n",
    "- Se usa un **scheduler lineal con warmup** para suavizar el entrenamiento.\n",
    "\n",
    "### Hiperparámetros Recomendados por los Autores de BERT\n",
    "\n",
    "| Hiperparámetro | Valor | Justificación |\n",
    "|---|---|---|\n",
    "| Batch size | 16 o 32 | Sugeridos en el paper original |\n",
    "| Learning rate | 2e-5, 3e-5, 5e-5 | Tasas bajas para preservar features |\n",
    "| Épocas | 2 a 4 | Pocas épocas porque el modelo ya sabe mucho |\n",
    "| Optimizer | AdamW | Adam con weight decay correcto |\n",
    "| Max seq length | 128 o 512 | Depende de la longitud del texto |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importación de Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\nimport time\nimport datetime\nimport random\nimport numpy as np\nimport pandas as pd\nimport torch\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TENSORS_DIR = \"../data/tensors/\"\n",
    "MODEL_SAVE_DIR = \"../data/model_save/\"\n",
    "MODEL_NAME = 'bert-base-uncased'\n",
    "BATCH_SIZE = 32          # Recomendado por BERT: 16 o 32\n",
    "EPOCHS = 3               # Entre 2 y 4 para fine-tuning\n",
    "LEARNING_RATE = 2e-5     # Tasa baja para no destruir features preentrenadas\n",
    "EPSILON = 1e-8           # Epsilon para AdamW\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuración del Dispositivo (GPU)\n",
    "\n",
    "Verificamos la GPU. Usamos una **RTX 5090 con 32 GB de VRAM**, suficiente para batch_size=32 y secuencias de 128 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_mem / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU no disponible, usando CPU.\")\n",
    "\n",
    "# Semilla para reproducibilidad\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Carga de los Datasets Tokenizados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torch.load(os.path.join(TENSORS_DIR, \"train_dataset.pt\"), weights_only=False)\n",
    "val_dataset = torch.load(os.path.join(TENSORS_DIR, \"val_dataset.pt\"), weights_only=False)\n",
    "print(f\"Train: {len(train_dataset):,} muestras\")\n",
    "print(f\"Val:   {len(val_dataset):,} muestras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Creación de DataLoaders\n",
    "\n",
    "- **Entrenamiento:** `RandomSampler` — mezcla aleatoriamente cada época.\n",
    "- **Validación:** `SequentialSampler` — recorre secuencialmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=BATCH_SIZE)\n",
    "validation_dataloader = DataLoader(val_dataset, sampler=SequentialSampler(val_dataset), batch_size=BATCH_SIZE)\n",
    "\n",
    "print(f\"Batches de entrenamiento: {len(train_dataloader)}\")\n",
    "print(f\"Batches de validación:    {len(validation_dataloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Carga del Modelo BERT\n",
    "\n",
    "Usamos `BertForSequenceClassification`: BERT base + capa lineal de clasificación.\n",
    "\n",
    "```\n",
    "Input → BERT Encoder (12 capas transformer) → [CLS] embedding → Linear(768, 2) → Salida\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Cargando modelo {MODEL_NAME}...\")\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME, num_labels=2, output_attentions=False, output_hidden_states=False,\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "params = list(model.named_parameters())\n",
    "print(f\"\\nEl modelo tiene {len(params)} grupos de parámetros.\")\n",
    "print(f\"\\n==== Capa de Embedding ====\")\n",
    "for p in params[0:5]:\n",
    "    print(f\"  {p[0]:<55} {str(tuple(p[1].size())):>12}\")\n",
    "print(f\"\\n==== Primer Transformer ====\")\n",
    "for p in params[5:21]:\n",
    "    print(f\"  {p[0]:<55} {str(tuple(p[1].size())):>12}\")\n",
    "print(f\"\\n==== Capa de Salida (Clasificación) ====\")\n",
    "for p in params[-4:]:\n",
    "    print(f\"  {p[0]:<55} {str(tuple(p[1].size())):>12}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Optimizer y Scheduler\n",
    "\n",
    "- **AdamW:** Adam con weight decay correcto. lr=2e-5 como recomiendan los autores de BERT.\n",
    "- **Linear Schedule con Warmup:** Reduce el learning rate linealmente desde el valor inicial hasta 0. Estabiliza el inicio del entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, eps=EPSILON)\n",
    "\n",
    "total_steps = len(train_dataloader) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "print(f\"Total de pasos: {total_steps:,}\")\n",
    "print(f\"Learning rate: {LEARNING_RATE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Funciones Auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_accuracy(preds, labels):\n",
    "    \"\"\"Calcula accuracy comparando predicciones vs labels.\"\"\"\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "def format_time(elapsed):\n",
    "    \"\"\"Formatea segundos a hh:mm:ss.\"\"\"\n",
    "    return str(datetime.timedelta(seconds=int(round(elapsed))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Bucle de Entrenamiento y Validación\n",
    "\n",
    "Sigue el patrón del notebook de clase (`BERT_Fine_Tuning.ipynb`):\n",
    "\n",
    "**Por cada época:**\n",
    "1. **Entrenamiento (`model.train()`):** Forward pass → loss → backward → gradient clipping → optimizer step → scheduler step.\n",
    "2. **Validación (`model.eval()`):** Forward pass sin gradientes → calcular loss y accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_stats = []\n",
    "total_t0 = time.time()\n",
    "\n",
    "for epoch_i in range(EPOCHS):\n",
    "    print(f'\\n{\"=\"*40}')\n",
    "    print(f'  Época {epoch_i + 1} / {EPOCHS}')\n",
    "    print(f'{\"=\"*40}')\n",
    "\n",
    "    # ---- ENTRENAMIENTO ----\n",
    "    print(\"\\nEntrenando...\")\n",
    "    t0 = time.time()\n",
    "    total_train_loss = 0\n",
    "    model.train()\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        if step % 40 == 0 and step != 0:\n",
    "            print(f'  Lote {step:>5,} de {len(train_dataloader):>5,}. Tiempo: {format_time(time.time() - t0)}')\n",
    "\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        model.zero_grad()\n",
    "        result = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask,\n",
    "                       labels=b_labels, return_dict=True)\n",
    "\n",
    "        loss = result.loss\n",
    "        total_train_loss += loss.item()\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient clipping: evita gradientes explosivos\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    training_time = format_time(time.time() - t0)\n",
    "    print(f\"\\n  Pérdida promedio: {avg_train_loss:.4f}\")\n",
    "    print(f\"  Tiempo: {training_time}\")\n",
    "\n",
    "    # ---- VALIDACIÓN ----\n",
    "    print(\"\\nValidando...\")\n",
    "    t0 = time.time()\n",
    "    model.eval()\n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "\n",
    "    for batch in validation_dataloader:\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            result = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask,\n",
    "                           labels=b_labels, return_dict=True)\n",
    "\n",
    "        total_eval_loss += result.loss.item()\n",
    "        logits = result.logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "\n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(f\"  Accuracy: {avg_val_accuracy:.4f}\")\n",
    "    print(f\"  Pérdida: {avg_val_loss:.4f}\")\n",
    "    print(f\"  Tiempo: {validation_time}\")\n",
    "\n",
    "    training_stats.append({\n",
    "        'epoch': epoch_i + 1, 'Training Loss': avg_train_loss,\n",
    "        'Valid. Loss': avg_val_loss, 'Valid. Accur.': avg_val_accuracy,\n",
    "        'Training Time': training_time, 'Validation Time': validation_time\n",
    "    })\n",
    "\n",
    "print(f\"\\nEntrenamiento completo! Tiempo total: {format_time(time.time() - total_t0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Resultados del Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stats = pd.DataFrame(data=training_stats).set_index('epoch')\n",
    "print(df_stats.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Guardado del Modelo\n",
    "\n",
    "Usamos `save_pretrained()` de HuggingFace que guarda los pesos (`model.safetensors`), configuración (`config.json`) y tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
    "model_to_save = model.module if hasattr(model, 'module') else model\n",
    "model_to_save.save_pretrained(MODEL_SAVE_DIR)\n",
    "BertTokenizer.from_pretrained(MODEL_NAME).save_pretrained(MODEL_SAVE_DIR)\n",
    "print(f\"Modelo y tokenizer guardados en: {MODEL_SAVE_DIR}\")\n",
    "\n",
    "df_stats.to_csv(os.path.join(MODEL_SAVE_DIR, \"training_stats.csv\"))\n",
    "print(\"Estadísticas guardadas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumen\n",
    "\n",
    "**Resultados obtenidos:**\n",
    "\n",
    "| Época | Training Loss | Valid. Loss | Valid. Accuracy |\n",
    "|---|---|---|---|\n",
    "| 1 | 0.300 | 0.234 | 90.7% |\n",
    "| 2 | 0.177 | 0.232 | 91.3% |\n",
    "| 3 | 0.107 | 0.299 | 91.1% |\n",
    "\n",
    "La mejor accuracy de validación se obtiene en la época 2 (91.3%). La validation loss aumenta en la época 3, indicando un inicio de overfitting.\n",
    "\n",
    "**Siguiente paso:** Evaluar en el conjunto de test."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}